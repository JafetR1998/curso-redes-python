{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "name": "",
  "signature": "sha256:f7168a2aacd7c845e593f72181487979b72f446f0ed7e7e1b96b90980a3bea81"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# An\u00e1lisis de Texto\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Mediante el an\u00e1lisis del contenido de los tweets podemos obtener informaci\u00f3n adicional, por ejemplo, los temas que se est\u00e1 tratando, si se comparten p\u00e1ginas web, etc. Existen m\u00f3dulos y herramientas especializadas para realizar an\u00e1lisis de texto, sin embargo en este curso s\u00f3lo utilizaremos las herramientas b\u00e1sicas de python para comparar cadenas de caracteres.\n",
      "\n",
      "Existen muchas variantes para este tipo de an\u00e1lisis, dependienedo de la clase de informaci\u00f3n que nos interese extraer; entre los m\u00e1s comunes destacamos el an\u00e1lisis de sentimiento, clasificadores de t\u00f3picos, clasificadores de entidades, etc. Sin embargo en todos los casos siempre es indispensable dotar de un formato al texto para agilizar eel an\u00e1lisis, por ejemplo remover acentos, convertir el texto a min\u00fasculas/may\u00fasculas, remover p\u00e1ginas web, etc. Comenzaremos esta secci\u00f3n extrayendo el texto de un conjunto de tweets y realizando limpieza sobre este texto (remover acentos y caracteres especiales).\n",
      "\n",
      "Si hemos seguido el notebook hasta este punto, debemos tener en la carpeta _data_ el archivo \u00a8stream_lunch.json\u00a8, el cual contiene un conjunto de tweets obtenidos a traves del stremming. \n",
      "\n",
      "Comencemos extrayendo el texto de todos los tweets, sustituyendo caract\u00e9res y guard\u00e1ndolo en un archivo independiente para facilitar una posible inspecci\u00f3n visual. El siguiente c\u00f3digo extraemos tweets con un cierto tema y ademas guarda en memoria una lista `Textos` con el contenido de cada tweet:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ahora leemos los tweets almacenado en el archivo `stream_lunch.json` y guardamos los textos en una lista llamada `Textos`. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "from unidecode import unidecode\n",
      "\n",
      "with open(\"./data/stream_lunch.json\",\"r\") as archivo:\n",
      "    renglones=archivo.readlines()\n",
      "\n",
      "Textos=[]\n",
      "with open(\"./data/Textos.txt\",\"w\") as archivo2:\n",
      "    for data in renglones:\n",
      "        tuit=json.loads(data)\n",
      "        texto=unidecode(tuit['text']) #\n",
      "        texto.encode('ascii')# Reemplazan caracteres por su equivalente ascii\n",
      "        Textos.append(texto)\n",
      "        archivo2.write(texto)\n",
      "        archivo2.write('\\n\\n')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Uno de los procesos que facilita an\u00e1lisis de texto es sustituir los caracteres con acento y signos de interrogaci\u00f3n y admiraci\u00f3n de apertura (\"\u00bf\",\"\u00a1\") por caracteres sin acento y s\u00f3lo signos de cierre \"?\" y \"!\". Esto lo hacemos porque no todos los usuarios incluyen acentos, signos iniciales \"\u00bf\",\"\u00a1\", p\u00e1ginas o may\u00fasculas.Podemos ver el efecto de sustituir los caracteres acentuados imprimiendo el texto original y el modificado"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "L=[json.loads(data)['text'] for data in renglones]\n",
      "for i in range(13):\n",
      "    print(L[i])\n",
      "    print(Textos[i])\n",
      "    print('\\n\\n')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Adem\u00e1s, podemos remover ligas a p\u00e1ginas web para que las palabras en ellas no interfieran en el an\u00e1lisis. Podemos remover los links que se comparten en el texto, por ejemplo:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "for i in range(10):\n",
      "    sin_url = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', Textos[i])\n",
      "    print(sin_url)\n",
      "    print('\\n\\n')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Repitamos el proceso de remover direcciones de p\u00e1ginas para todos los textos y los almacen\u00e9moslos de esta manera"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "Textos=[re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', texto) for texto in Textos]    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(Textos[:10])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Adicionalmente, resulta conveniente reemplazar todas las letras may\u00fasculas por min\u00fasculas, pues recordemos que Python las interpreta como caracteres distintos, esto lo podemos hcer mediante el siguiente c\u00f3digo:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Textos=[texto.lower() for texto in Textos]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Independientemente del tipo de an\u00e1lisis que nos interese realizar, el contenido del texto lo analizaremos a partir de las palabras que aparecen en \u00e9l, sin embargo no todas las palabras tienen la misma importancia, por ejemplo, los art\u00edculos, pronombres, preposiciones, etc no son palabras representativas del contenido, y por esta raz\u00f3n a estas palabras com\u00fanmente se les denomina _palabras vac\u00edas_ \u00f3 _stop words_. En la carpeta _data_ se encuentra una lista de _palabras vac\u00edas_ que puede extenderse y adaptarse. El siguiente c\u00f3digo lee y almacena en una lista las _palabras stop_ para posteriormente utilizarlas."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#palabras_vacias='./data/stop_words.txt'\n",
      "with open('./data/stopwordES.txt',\"r\") as archivo:\n",
      "    palabras_stop=archivo.readlines()\n",
      "    \n",
      "palabras_stop=[palabra[:-1] for palabra in palabras_stop]\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(palabras_stop)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Adem\u00e1s de las palabras _vac\u00edas_ o _stop_, podemos crear listas adicionales con palabras asociadas a un tema. Aunque etas listas las podemos proponer para diversos tipo an\u00e1lisis, no en todos resulta f\u00e1cil proponer una lista \u00fatil de palabras, por ejemplo para un an\u00e1lsis de sentimiento podr\u00edamos tratar de discriminar aquellos tweets con una connotaci\u00f3n positiva, en este caso la lista podr\u00eda incluir las palabras 'amanecer', 'alegria', 'reir', etc; aunque es posible que sea m\u00e1s complicado proponer este listado debido a la complejidad o variabilidad de los temas en los tweets que recolectamos. \n",
      "\n",
      "Sin embargo, si recolectamos los tweets buscando un _hashtag_ o una palabra en particular, tenemos una idea de qu\u00e9 palabras pueden estar presentes. Por ejemplo, si recolectamos tweets buscando la palabra \"accidente\", muchos de los tweets pueden contener adem\u00e1s la palabra \"automovil\", \"derrumbe\", etc, y de igual manera evitar los tweets que contenga otras palabras relacionadas con otros temas. Por ejemplo, para ver cu\u00e1ntos de los tweets cuyo mensaje almacenamos en la variable ``Textos` contienen palabras asociadas a un tema, podemos usar el siguiente c\u00f3digo:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "semantica=['ma\u00f1ana','dia','amanecer']\n",
      "contador=0\n",
      "\n",
      "for texto in Textos:\n",
      "    for palabra in semantica:\n",
      "        if palabra in texto:\n",
      "             contador=contador+1\n",
      "print(contador)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Nota**: Recuerda que buscamos palabras con letras may\u00fasculas nunca las vamos a encontrar, pues hemos sustitutido letras may\u00fasculas por min\u00fasculas\n",
      "\n",
      "**Ejercicio:** Modifica las palabras a buscar de acuerdo a tus intereses y los tweets recolectados."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Un an\u00e1lisis m\u00e1s fino requiere analizar palabra por palabra, as\u00ed podr\u00edamos identificar las palabras de mayor frecuencia. Primero debemos dividir cada texto en palabras, y despu\u00e9s almacenar las palabras que no sean _stop_ junto con el n\u00famero de veces que es usada. Para agilizar este progeso, s\u00f3lo compararemos palabras que no inicien con caracteres num\u00e9ricos o caracteres _especiales_ (\"@\" , \"#\", \"$\",\"%\"). Veamos un ejemplo."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Creamos un diccionario de las palabras que no son stop\n",
      "diccionario={}\n",
      "\n",
      "# Podemos optar por incluir o no los '#'\n",
      "\n",
      "for texto in Textos:\n",
      "    t=texto.split(' ') # separa por cadena de texto por palabras y las guarda en una lista t\n",
      "    for palabra in t:\n",
      "        if palabra != '':\n",
      "            if (palabra not in palabras_stop) and (palabra[0] not in range(10)) and (palabra[0] not in [\"@\",\"#\",\"$\",\"%\",\"/\",\"\\n\",\"!\",\"?\"]) :\n",
      "                diccionario[palabra]=0\n",
      "\n",
      "            \n",
      "#Corremos un codigo similar pero ahora realiza el conteo de las apariciones de las palabras en el diccionario\n",
      "for texto in Textos:\n",
      "    t=texto.split(' ') \n",
      "    for palabra in t:\n",
      "        if palabra != '':\n",
      "            if (palabra not in palabras_stop) and (palabra[0] not in range(10)) and (palabra[0] not in [\"@\",\"#\",\"$\",\"%\",\"/\",\"\\n\",\"!\",\"?\"]) :\n",
      "                diccionario[palabra]=diccionario[palabra]+1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Inspeccionemos el diccionario buscando los valores de ocurrencia m\u00e1s altas:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "valores=[diccionario[valor] for valor in diccionario.keys()] # Lista los valores de cada palabra en el diccionario\n",
      "valores.sort()  #Ordena la lista de menor a mayor \n",
      "valores.reverse() #invierte el orden de la lista \n",
      "\n",
      "print(valores[:10])  #imprime los 10 valores m\u00e1s altos"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ahora que sabemos el rango de ocurrencia de las palabras, busquemos en el diccionario aquellas palabras con un \u00edndice alto:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alta_ocurr=[]\n",
      "\n",
      "for palabra in diccionario:\n",
      "    if diccionario[palabra]>= : #Debemos incluir un valor numerico por el valor en la frecuencia que nos interese encontrar\n",
      "        alta_ocurr.append(palabra)\n",
      "        \n",
      "print(alta_ocurr)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Si nos interesan saber cuales son todas las palabras que encontramos solo debemos escribir lo siguiente:  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "print(diccionario.keys())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Con este diccionario podemos darnos una idea de qu\u00e9 palabras considerar dentro del listado, y una manera de optimizarlo es considerar s\u00f3lo la ra\u00edz de la palabra que nos interese (cuando sea posible), por ejemplo si encontramos 'accidentes', 'accidentado', 'accidentada', etc podemos preguntarnos solamente si 'accident' est\u00e1 incluido en las palabras que conforman el texto de cada tweet."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lista=['accident']\n",
      "textos_filtrados=[] # Aqui almacenaremos los textos que incluyan las palabras de la lista\n",
      "\n",
      "for texto in Textos:\n",
      "    t=texto.split(' ') \n",
      "    for palabra in t:\n",
      "        if palabra != '':\n",
      "            for raiz in lista:\n",
      "                if raiz in palabra:\n",
      "                    textos_filtrados.append(texto)\n",
      "                    break # solo comprueba si hay una coincidencia\n",
      "\n",
      "#Contamos cuantos tweets continen al menos una de las raices de palabras en 'lista'\n",
      "print(len(textos_filtrados))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
